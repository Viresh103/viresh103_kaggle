{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import gc\n",
    "# from itertools import product\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline \n",
    "\n",
    "pd.set_option('display.max_rows', 600)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "\n",
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):\n",
    "    tmp = df[['date_block_num','shop_id','item_id', col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    del tmp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=pd.read_csv(\"sales_train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = []\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import sklearn\n",
    "import scipy \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Grouproduct(*[cur_shops, cur_itemspby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'item_id'],as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "\n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=['shop_id', 'item_id']).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat=pd.read_csv('item_cat.csv')\n",
    "shop=pd.read_csv('shop_city.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "all_data=pd.merge(all_data,item_cat,on=['item_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=pd.merge(all_data,shop,on=['shop_id'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)\n",
    "########## 1. Create 'date_item_city_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'item_id', 'city_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_item_city_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'item_id', 'city_code'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_item_city_avg_item_cnt')\n",
    "all_data.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(2)\n",
    "########## 2. Create 'date_item_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'item_id']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_item_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num','item_id'], how='left')\n",
    "all_data = lag_feature(all_data, [1, 2, 3], 'date_item_avg_item_cnt')\n",
    "all_data.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(3)\n",
    "########## 3. Create 'date_shop_type_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_shop_type_avg_item_cnt']    ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'shop_id', 'type_code'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_shop_type_avg_item_cnt')\n",
    "all_data.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(4)\n",
    "########## 4. Create 'date_shop_cat_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_shop_cat_avg_item_cnt']    ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_shop_cat_avg_item_cnt')\n",
    "all_data.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(5)\n",
    "########## 5. Create 'date_subtype_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'subtype_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_subtype_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'subtype_code'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_subtype_avg_item_cnt')\n",
    "all_data.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
    "all_data.head()\n",
    "\n",
    "print(6)\n",
    "########## 6. Create 'date_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_avg_item_cnt']          ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_avg_item_cnt')\n",
    "all_data.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(7)\n",
    "########## 7. Create 'date_type_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'type_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_type_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'type_code'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_type_avg_item_cnt')\n",
    "all_data.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(8)\n",
    "########## 8. Create 'date_shop_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'shop_id']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_shop_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num','shop_id'], how='left')\n",
    "all_data = lag_feature(all_data, [1, 2], 'date_shop_avg_item_cnt')\n",
    "all_data.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(9)\n",
    "########## 9. Create 'date_city_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'city_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_city_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'city_code'], how='left')\n",
    "all_data = lag_feature(all_data, [1], 'date_city_avg_item_cnt')\n",
    "all_data.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean encoding item id\")\n",
    "#####mean encoding item id\n",
    "item_id_target_mean = all_data.groupby('item_id').target.mean()\n",
    "all_data['item_target_enc'] = all_data['item_id'].map(item_id_target_mean)\n",
    "all_data['item_target_enc'].fillna(0.3343, inplace=True) \n",
    "all_data=all_data.drop(['item_id'],axis=1)\n",
    "\n",
    "print(\"mean encoding shop id\")\n",
    "#####mean encoding shop id\n",
    "shop_id_target_mean=all_data.groupby('shop_id').target.mean()\n",
    "all_data['shop_target_enc']=all_data['shop_id'].map(shop_id_target_mean)\n",
    "all_data['shop_target_enc'].fillna(0.3343, inplace=True)\n",
    "all_data=all_data.drop(['shop_id'],axis=1)\n",
    "\n",
    "print(\"mean encoding item_category_id\")\n",
    "#####mean encoding item_category_id\n",
    "item_cat_id_target_mean=all_data.groupby('item_category_id').target.mean()\n",
    "all_data['item_cat_target_enc']=all_data['item_category_id'].map(item_cat_id_target_mean)\n",
    "all_data['item_cat_target_enc'].fillna(0.3343, inplace=True)\n",
    "all_data=all_data.drop(['item_category_id'],axis=1)\n",
    "\n",
    "print(\"mean encoding type_code\")\n",
    "#####mean encoding type_code\n",
    "type_code_target_mean=all_data.groupby('type_code').target.mean()\n",
    "all_data['type_code_target_enc']=all_data['type_code'].map(type_code_target_mean)\n",
    "all_data['type_code_target_enc'].fillna(0.3343, inplace=True)\n",
    "all_data=all_data.drop(['type_code'],axis=1)\n",
    "\n",
    "print(\"mean encoding subtype_code\")\n",
    "#####mean encoding subtype_code\n",
    "subtype_code_target_mean=all_data.groupby('subtype_code').target.mean()\n",
    "all_data['subtype_code_target_enc']=all_data['subtype_code'].map(subtype_code_target_mean)\n",
    "all_data['subtype_code_target_enc'].fillna(0.3343, inplace=True)\n",
    "all_data=all_data.drop(['subtype_code'],axis=1)\n",
    "\n",
    "print(\"mean encoding city_code\")\n",
    "#####mean encoding city_code\n",
    "city_code_target_mean=all_data.groupby('city_code').target.mean()\n",
    "all_data['city_code_target_enc']=all_data['city_code'].map(city_code_target_mean)\n",
    "all_data['city_code_target_enc'].fillna(0.3343, inplace=True)\n",
    "all_data=all_data.drop(['city_code'],axis=1)\n",
    "\n",
    "all_data = downcast_dtypes(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"all_data2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test datapreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tlag_feature(test, lags, col):\n",
    "    tmp = all_data[['date_block_num','shop_id','item_id', col]]\n",
    "    for i in lags:\n",
    "        shifted = tmp.copy()\n",
    "        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n",
    "        shifted['date_block_num'] += i\n",
    "        test= pd.merge(test, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    del tmp\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')\n",
    "date_block_num=[34 for i in range(214200)]\n",
    "date_block_num=pd.DataFrame(np.vstack(date_block_num), columns = ['date_block_num'],dtype=np.int32)\n",
    "test=pd.concat([test,date_block_num] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_cat=pd.read_csv('item_cat.csv')\n",
    "shop=pd.read_csv('shop_city.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.merge(test,item_cat,on=['item_id'],how='left')\n",
    "test=pd.merge(test,shop,on=['shop_id'],how='left')\n",
    "del item_cat,shop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)\n",
    "########## 1. Create 'date_item_city_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'item_id', 'city_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_item_city_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'item_id', 'city_code'], how='left')\n",
    "test = tlag_feature(test, [1], 'date_item_city_avg_item_cnt')\n",
    "all_data.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(2)\n",
    "########## 2. Create 'date_item_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'item_id']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_item_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num','item_id'], how='left')\n",
    "test = tlag_feature(test, [1, 2, 3], 'date_item_avg_item_cnt')\n",
    "all_data.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(3)\n",
    "########## 3. Create 'date_shop_type_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_shop_type_avg_item_cnt']    ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'shop_id', 'type_code'], how='left')\n",
    "test = tlag_feature(test, [1], 'date_shop_type_avg_item_cnt')\n",
    "all_data.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "print(4)\n",
    "########## 4. Create 'date_shop_cat_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_shop_cat_avg_item_cnt']    ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\n",
    "test = tlag_feature(test,[1], 'date_shop_cat_avg_item_cnt')\n",
    "all_data.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(5)\n",
    "########## 5. Create 'date_subtype_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'subtype_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_subtype_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'subtype_code'], how='left')\n",
    "test = tlag_feature(test,all_data, [1], 'date_subtype_avg_item_cnt')\n",
    "all_data.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\n",
    "all_data.head()\n",
    "\n",
    "print(6)\n",
    "########## 6. Create 'date_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_avg_item_cnt']          ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num'], how='left')\n",
    "test= tlag_feature(test, [1], 'date_avg_item_cnt')\n",
    "all_data.drop(['date_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n",
    "print(7)\n",
    "########## 7. Create 'date_type_avg_item_cnt'\n",
    "temp = all_data.groupby(['date_block_num', 'type_code']).agg({'target': ['mean']})\n",
    "temp.columns = ['date_type_avg_item_cnt']     ###\n",
    "temp.reset_index(inplace=True)\n",
    "all_data = pd.merge(all_data, temp, on=['date_block_num', 'type_code'], how='left')\n",
    "test = tlag_feature(test, [1], 'date_type_avg_item_cnt')\n",
    "all_data.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean encoding item id\")\n",
    "#####mean encoding shop id\n",
    "# shop_id_target_mean=test.groupby('shop_id').target.mean()\n",
    "test['item_target_enc']=test['item_id'].map(shop_id_target_mean)\n",
    "test['item_target_enc'].fillna(0.3343, inplace=True)\n",
    "test=test.drop(['item_id'],axis=1)\n",
    "\n",
    "print(\"mean encoding shop id\")\n",
    "#####mean encoding shop id\n",
    "# shop_id_target_mean=test.groupby('shop_id').target.mean()\n",
    "test['shop_target_enc']=test['shop_id'].map(shop_id_target_mean)\n",
    "test['shop_target_enc'].fillna(0.3343, inplace=True)\n",
    "test=test.drop(['shop_id'],axis=1)\n",
    "\n",
    "print(\"mean encoding item_category_id\")\n",
    "#####mean encoding item_category_id\n",
    "# item_cat_id_target_mean=test.groupby('item_category_id').target.mean()\n",
    "test['item_cat_target_enc']=test['item_category_id'].map(item_cat_id_target_mean)\n",
    "test['item_cat_target_enc'].fillna(0.3343, inplace=True)\n",
    "test=test.drop(['item_category_id'],axis=1)\n",
    "\n",
    "print(\"mean encoding type_code\")\n",
    "#####mean encoding type_code\n",
    "# type_code_target_mean=test.groupby('type_code').target.mean()\n",
    "test['type_code_target_enc']=test['type_code'].map(type_code_target_mean)\n",
    "test['type_code_target_enc'].fillna(0.3343, inplace=True)\n",
    "test=test.drop(['type_code'],axis=1)\n",
    "\n",
    "print(\"mean encoding subtype_code\")\n",
    "#####mean encoding subtype_code\n",
    "# subtype_code_target_mean=test.groupby('subtype_code').target.mean()\n",
    "test['subtype_code_target_enc']=test['subtype_code'].map(subtype_code_target_mean)\n",
    "test['subtype_code_target_enc'].fillna(0.3343, inplace=True)\n",
    "test=test.drop(['subtype_code'],axis=1)\n",
    "\n",
    "print(\"mean encoding city_code\")\n",
    "#####mean encoding city_code\n",
    "# city_code_target_mean=test.groupby('city_code').target.mean()\n",
    "test['city_code_target_enc']=test['city_code'].map(city_code_target_mean)\n",
    "test['city_code_target_enc'].fillna(0.3343, inplace=True)\n",
    "test=test.drop(['city_code'],axis=1)\n",
    "\n",
    "test = downcast_dtypes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"modified_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
